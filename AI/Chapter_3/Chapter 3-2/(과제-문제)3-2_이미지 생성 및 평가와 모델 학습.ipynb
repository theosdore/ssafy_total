{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9u2lnjI8fH6-v2"
      },
      "source": [
        "### **Content License Agreement**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : 본 자료는 삼성청년SW·AI아카데미의 컨텐츠 자산으로, 보안서약서에 의거하여 어떠한 사유로도 임의로 복사, 촬영, 녹음, 복제, 보관, 전송하거나 허가 받지 않은 저장매체를 이용한 보관, 제3자에게 누설, 공개 또는 사용하는 등의 무단 사용 및 불법 배포 시 법적 조치를 받을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af5r70FJ9Fbr-hw-v2"
      },
      "source": [
        "# **Objectives**\n",
        "\n",
        "1. 과제 개요\n",
        "  - 본 과제는 이미지 파운데이션 모델(Image Foundation Models)의 활용과 비교를 중심으로 Text-to-Image 생성, CLIP 기반 이미지 평가, CNN 기반 모델(ResNet) 비교, 생성 데이터로 ResNet18 Fine-tuning 의 전체 워크플로우를 직접 구현하는 것을 목표로 합니다.\n",
        "\n",
        "2. 과제 진행 목적 및 배경\n",
        "  - 파운데이션 모델 응용: Stable Diffusion, CLIP, ResNet 등 다양한 사전 학습 모델을 활용하여, 실습에서 배운 개념을 **'클래식 자동차'**라는 새로운 주제에 적용합니다.\n",
        "  - 멀티모달 평가 구현: 텍스트와 이미지를 함께 다루는 CLIP 모델을 활용하여, **복합적인 상황(객체, 배경, 분위기)을 묘사한** 이미지의 의미적 적합성을 주도적으로 평가합니다.\n",
        "  - 전이 학습(Transfer Learning)과 데이터셋 구축: Stable Diffusion으로 **객체의 특정 '상태'(새것 vs 낡음)를 표현하는** 이미지 데이터셋을 직접 구축하고, 이를 ResNet18 모델 학습에 활용하여 생성 데이터의 가치를 탐구하고 데이터셋 구축 능력을 기릅니다.\n",
        "  - 모델 비교 분석 능력 심화: CLIP과 전통 CNN의 분류 결과를 다른 예시에서 비교하여 모델 특성과 성능 차이에 대한 이해를 공고히 합니다.\n",
        "\n",
        "3. 과제 수행으로 얻어갈 수 있는 역량\n",
        "  - 프롬프트 엔지니어링: 특정 컨셉(예: 해안 도로를 달리는 클래식 카)을 구현하기 위한 효과적인 positive/negative prompt 작성 능력.\n",
        "  - 멀티모달 평가:\n",
        "    - CLIP을 사용하여 이미지와 직접 정의한 텍스트 레이블 간 유사도를 측정하고 해석하는 기술.\n",
        "    - CLIP 모델과 ResNet 모델의 결과를 비판적으로 비교 분석하는 능력.\n",
        "  - CNN 활용 능력: ResNet 계열 모델을 불러와 전이 학습 및 Fine-tuning을 처음부터 구현하는 능력.\n",
        "  - 데이터 생성 및 확장 전략: **특정 상태(condition) 분류** 목적에 맞는 합성 데이터셋을 직접 제작하고 활용하는 전략 수립 능력.\n",
        "\n",
        "\n",
        "4. 과제 핵심 내용\n",
        "  - HuggingFace Stable Diffusion 모델로 **'해안 도로의 클래식 자동차'** 컨셉 이미지 생성\n",
        "  - CLIP 모델로 생성된 자동차 이미지와 **시대, 스타일, 배경을 포함한 텍스트 레이블** 간의 의미적 유사도 평가\n",
        "  - ResNet-50으로 동일 이미지를 분류한 후 CLIP의 결과와 비교 분석\n",
        "  - Stable Diffusion으로 **'새 차'와 '녹슨 폐차'** 합성 데이터셋을 직접 생성하여 ResNet-18 미세조정 (Fine-Tuning) 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Prerequisites**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```\n",
        "pytorch : 2.7.1\n",
        "torchvision: 0.22.1\n",
        "transformers:  4.55.1\n",
        "datasets:  4.0.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK367VEh9E6u-hw-v2"
      },
      "source": [
        "실습과 동일하게 아래 라이브러리들이 필요합니다. 설치되어 있지 않다면 셀을 실행하여 설치해주세요.\n",
        "```bash\n",
        "!pip install torch torchvision diffusers transformers accelerate\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zqZoW7JrMOk-hw-v2"
      },
      "source": [
        "\n",
        "# **Assignment Overview**\n",
        "\n",
        "### **들어가며: 파운데이션 모델 (Foundation Model) 복습**\n",
        "\n",
        "대규모 데이터로 사전 학습된 범용 인공지능 모델로, 다양한 다운스트림 태스크에 전이하여 효율적으로 사용할 수 있습니다. 이번 과제에서는 실습에서 다룬 Stable Diffusion, CLIP, ResNet을 다시 한번 활용하여 새로운 문제를 해결합니다.\n",
        "\n",
        "### **과제 목차**\n",
        "\n",
        "이 노트북은 네 가지 주요 파트로 구성되어 있습니다.\n",
        "\n",
        "1.  **Stable Diffusion 모델로 컨셉 이미지 생성**: '해질녘 해안 도로를 달리는 1960년대 빨간색 컨버터블'을 주제로 사실적인 사진 스타일의 이미지를 생성합니다.\n",
        "2.  **CLIP 모델로 생성 이미지 평가**: 생성된 자동차 이미지가 의도한 컨셉(객체, 시대, 배경)과 얼마나 부합하는지 텍스트 레이블을 직접 만들어 평가합니다.\n",
        "3.  **ResNet-50으로 동일 이미지 분류 후 결과 비교**: 전통적인 CNN 모델인 ResNet-50의 분류 결과와 CLIP의 평가 결과를 비교하며 두 모델의 장단점을 분석합니다.\n",
        "4.  **합성 데이터셋으로 ResNet-18 미세조정**: Stable Diffusion으로 **'새 차'**와 **'녹슨 폐차'** 클래스 데이터셋을 직접 구축하고, 이를 이용해 ResNet-18 모델을 Fine-tuning하여 자동차의 상태를 분류하는 모델을 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0tHgw9t5AkU-hw-v2"
      },
      "source": [
        "먼저 과제 전체의 재현성을 위하여 PyTorch, NumPy, 그리고 Python의 random 모듈에 대한 시드를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7oD_ZKu41lQ-hw-v2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **0. Huggingface Inference API 활용 이미지 생성**\n",
        "\n",
        "HuggingFace의 Inference API를 사용하여 별도의 모델 구동 없이 이미지를 생성합니다. 개인용 Read API 키 발급에 문제가 있다면, 진행하지 않으셔도 좋습니다.\n",
        "이후 설명 될 Pipeline에 비해 사용 난이도가 낮고 별도의 뉴럴네트워크를 직접 구동하지 않아 GPU 연산을 필요로 하지 않지만, 제어할 수 있는 부분이 적어 여러가지 커스터마이징이 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import huggingface_hub\n",
        "\n",
        "#https://huggingface.co/settings/tokens 에서 Read 권한으로 Token을 발급받고, 해당 토큰을 아래에 넣어주세요.\n",
        "api_key = ''\n",
        "prompt = 'A legendary swordsman holding his sword.'\n",
        "client = InferenceClient(\n",
        "    provider=\"nebius\",\n",
        "    api_key=api_key,\n",
        ")\n",
        "\n",
        "\n",
        "# output is a PIL.Image object\n",
        "image = client.text_to_image(\n",
        "    prompt,\n",
        "    model=\"black-forest-labs/FLUX.1-dev\",\n",
        ")\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piQVxOharpTN-hw-v2"
      },
      "source": [
        "## **1. 텍스트-투-이미지 생성**\n",
        "\n",
        "HuggingFace의 `diffusers` 라이브러리를 사용하여 Stable Diffusion 파이프라인을 로드하고, **'해안 도로의 클래식 카'**를 주제로 이미지를 생성합니다. \n",
        " - **Positive Prompt**: 생성하고 싶은 이미지(자동차 모델/시대, 색상, 배경, 분위기 등)를 상세히 기술합니다.\n",
        " - **Negative Prompt**: 생성 이미지에서 제외하고 싶은 요소(예: 만화 스타일, 흐릿함, 왜곡 등)를 기술합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjcGgq0QldJL-hw-v2-problem"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"사용 중인 장치:\", device)\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# 문제 1\n",
        "# [TODO] Stable Diffusion 파이프라인을 로드하고 GPU로 이동시키는 코드를 작성하세요.\n",
        "# 힌트: from_pretrained 메소드를 사용하고, 메모리 효율을 위해 torch_dtype을 float16으로 설정합니다.\n",
        "pipe = ... # 코드를 작성하세요\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# 문제 2\n",
        "# [TODO] '해질녘 해안 도로를 달리는 1960년대 빨간색 컨버터블'을 주제로 positive와 negative 프롬프트를 작성하세요.\n",
        "# 힌트: 'photorealistic', '1960s', 'convertible', 'coastal road', 'sunset' 등의 키워드를 positive에 활용해 보세요.\n",
        "positive_prompt = \"...\" # 코드를 작성하세요\n",
        "negative_prompt = \"...\" # 코드를 작성하세요\n",
        "\n",
        "\n",
        "# 문제 3\n",
        "# [TODO] 위에서 작성한 프롬프트를 사용하여 이미지를 4장 생성하는 코드를 작성하세요.\n",
        "# 힌트: pipe 객체를 호출하며, guidance_scale, num_inference_steps, num_images_per_prompt 인자를 설정합니다.\n",
        "result = ... # 코드를 작성하세요\n",
        "images = result.images\n",
        "\n",
        "print(\"--- 생성된 이미지 --- \")\n",
        "for i, img in enumerate(images):\n",
        "    img.save(f\"generated_car_image_{i}.png\")\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKf2m9BsxIUC-hw-v2"
      },
      "source": [
        "## **2. CLIP 모델을 사용한 생성 이미지 평가**\n",
        "\n",
        "방금 생성한 이미지 중 하나가 여러분이 의도한 컨셉과 얼마나 잘 부합하는지 CLIP 모델로 평가합니다. 생성된 '클래식 카' 이미지를 올바르게 설명하는 **정답 레이블**과, **시대/스타일/배경**이 다른 **오답 레이블**들을 직접 정의하여 모델의 이해도를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51f597e3fc484ee7b9743c4c5bc3f623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b706c624f2548c5ae3e402a5b4c92f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf693b12bfea4095af173927df17bb46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40086490a5bd4ddcaec1ae76a427dc36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe9b26241b4d437cbdd1a2296e5ca30d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1962ac8e55ae45a086277db4dd14445c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee87a00769824f74bb412d04fc43f0d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0000838ae4bc4323a194ea5f5c4b01d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIP 예측 결과: 'a photo of a 1960s red convertible on a coastal road' 라벨이 가장 타당하다고 예측되었습니다.\n",
            "\n",
            "--- 각 레이블 별 확률 --- \n",
            "a photo of a 1960s red convertible on a coastal road: 0.9964\n",
            "a photo of a modern red sedan in a city: 0.0000\n",
            "a sketch of a classic red car: 0.0001\n",
            "a photo of a red convertible in a forest: 0.0035\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "image = Image.open(\"generated_car_image_0.png\")\n",
        "\n",
        "# 문제 4 \n",
        "# [TODO] 생성된 이미지를 평가하기 위한 텍스트 레이블을 작성하세요.\n",
        "# 힌트: 정답(1960년대, 컨버터블, 해안도로), 오답(다른 시대, 다른 차종, 다른 스타일, 다른 배경)을 포함하여 4개 이상의 레이블을 만드세요.\n",
        "labels = [\n",
        "    \"...\", # 정답 예시: \"a photo of a 1960s red convertible on a coastal road\"\n",
        "    \"...\", # 오답 예시\n",
        "    \"...\", # 오답 예시\n",
        "    \"...\"  # 오답 예시\n",
        "]\n",
        "\n",
        "# 문제 5 \n",
        "# [TODO] CLIP 모델과 프로세서를 로드하는 코드를 작성하세요.\n",
        "# 힌트: 모델 ID는 'openai/clip-vit-base-patch32' 입니다. processor와 model을 모두 로드해야 합니다.\n",
        "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
        "processor = ... # 코드를 작성하세요\n",
        "model_clip = ... # 코드를 작성하세요\n",
        "model_clip.to(device)\n",
        "\n",
        "# 문제 6\n",
        "# [TODO] 이미지와 텍스트 레이블을 전처리하는 코드를 작성하세요.\n",
        "# 힌트: processor를 사용하며, text와 images 인자를 모두 전달하고 결과를 pytorch 텐서로 받습니다.\n",
        "inputs = ... # 코드를 작성하세요\n",
        "inputs.to(device)\n",
        "\n",
        "\n",
        "# 문제 7\n",
        "# [TODO] 전처리된 입력을 CLIP 모델에 통과시켜 유사도 점수(logits)와 확률을 계산하는 코드를 작성하세요.\n",
        "# 힌트: torch.no_grad() 블록 안에서 모델을 실행하고, 결과에서 logits_per_image를 추출한 뒤 softmax를 적용합니다.\n",
        "with torch.no_grad():\n",
        "    outputs = ... # 코드를 작성하세요\n",
        "    logits_per_image = ... # 코드를 작성하세요\n",
        "    probs = ... # 코드를 작성하세요\n",
        "\n",
        "best_idx = logits_per_image.argmax(dim=1).item()\n",
        "print(f\"CLIP 예측 결과: '{labels[best_idx]}' 라벨이 가장 타당하다고 예측되었습니다.\")\n",
        "print(\"\\n--- 각 레이블 별 확률 --- \")\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"{label}: {probs[0][i].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0AdcEyUhGN-hw-v2"
      },
      "source": [
        "## **3. ResNet50 기반 모델과의 결과 비교**\n",
        "\n",
        "동일한 클래식 카 이미지를 전통적인 CNN 분류 모델인 ResNet-50으로 분류하고, CLIP의 결과와 비교합니다. ResNet-50은 ImageNet 카테고리(예: 'convertible', 'sports car')로 예측할 것입니다. CLIP이 '1960년대', '해안 도로', '해질녘'과 같은 문맥 정보를 이해하는 반면, ResNet-50은 주로 이미지의 핵심 객체 자체에만 집중하여 예측하는 경향이 있습니다. 이 차이를 직접 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            " 56%|█████▌    | 54.6M/97.8M [00:00<00:00, 192MB/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8c0acadb631469a8fc4ee9a81582760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 163MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ResNet-50 Top-5 예측 결과 ---\n",
            "1. 클래스: convertible (인덱스: 511), 확률: 0.4983\n",
            "2. 클래스: sports car (인덱스: 817), 확률: 0.0415\n",
            "3. 클래스: promontory (인덱스: 976), 확률: 0.0171\n",
            "4. 클래스: car wheel (인덱스: 479), 확률: 0.0054\n",
            "5. 클래스: grille (인덱스: 581), 확률: 0.0040\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "\n",
        "# 문제 8\n",
        "# [TODO] ImageNet으로 사전 학습된 ResNet-50 모델을 로드하고, 평가 모드로 설정하는 코드를 작성하세요.\n",
        "# 힌트: models.resnet50 함수와 weights 인자를 사용합니다. .eval()을 호출해야 합니다.\n",
        "resnet50 = ... # 코드를 작성하세요\n",
        "resnet50.eval()\n",
        "\n",
        "imagenet_classes = models.ResNet50_Weights.IMAGENET1K_V2.meta[\"categories\"]\n",
        "\n",
        "# 이미지를 ResNet-50에 맞게 전처리하는 코드입니다. (수정 불필요)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "img_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "# 문제 9\n",
        "# [TODO] 전처리된 이미지 텐서로 ResNet-50 예측을 수행하고, 확률 값을 계산하는 코드를 작성하세요.\n",
        "# 힌트: torch.no_grad() 블록 안에서 모델을 실행하고, 결과에 softmax 함수를 적용합니다.\n",
        "with torch.no_grad():\n",
        "    output = ... # 코드를 작성하세요\n",
        "    probs_resnet = ... # 코드를 작성하세요\n",
        "\n",
        "top5_prob, top5_idx = torch.topk(probs_resnet, 5)\n",
        "\n",
        "print(\"--- ResNet-50 Top-5 예측 결과 ---\")\n",
        "for i in range(top5_prob.size(0)):\n",
        "    class_idx = top5_idx[i].item()\n",
        "    class_name = imagenet_classes[class_idx]\n",
        "    probability = top5_prob[i].item()\n",
        "    print(f\"{i+1}. 클래스: {class_name} (인덱스: {class_idx}), 확률: {probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3o6FaPak6GB-hw-v2"
      },
      "source": [
        "## **4. 생성한 데이터로 ResNet18 모델 미세조정**\n",
        "\n",
        "이제 과제의 마지막 단계로, Stable Diffusion을 이용해 **동일한 자동차의 두 가지 다른 '상태'**를 표현하는 합성 데이터셋을 구축합니다. **'아주 깨끗한 새 차 (brand_new)'**와 **'오래되어 녹슨 폐차 (rusty_abandoned)'** 이미지를 각각 생성하여, 자동차의 상태를 분류하는 모델을 Fine-tuning 해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd1jWVptTvft-hw-v2-problem"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# 문제 10\n",
        "# [TODO] '새 차'와 '녹슨 폐차' 클래스에 대한 프롬프트를 각각 정의하세요.\n",
        "# 힌트: 'brand new', 'shiny', 'showroom' 등의 키워드와 'rusty', 'abandoned', 'wrecked', 'overgrown' 등의 키워드를 활용하여 상태를 극대화하세요.\n",
        "classes = {\n",
        "    \"brand_new\": \"...\", # 코드를 작성하세요\n",
        "    \"rusty_abandoned\": \"...\" # 코드를 작성하세요\n",
        "}\n",
        "neg_prompt_ft = \"cartoon, drawing, sketch, text, watermark\"\n",
        "\n",
        "# 학습 및 테스트 데이터 폴더 생성\n",
        "for split in ['train', 'test']:\n",
        "    for cls in classes.keys():\n",
        "        os.makedirs(f\"data/{split}/{cls}\", exist_ok=True)\n",
        "# 문제 10 - 2        \n",
        "# [TODO] 각 클래스별로 학습 이미지 5장, 테스트 이미지 3장을 생성하는 코드를 작성하세요.\n",
        "\n",
        "# 각 클래스별로 학습 이미지 5장, 테스트 이미지 3장을 생성합니다. (수정 불필요)\n",
        "for split in ['train', 'test']:\n",
        "    num_images = 5 if split == 'train' else 3\n",
        "    for cls, prompt in classes.items():\n",
        "        print(f\"--- Generating {split} images for {cls}... ---\")\n",
        "        result = pipe(\n",
        "            prompt, \n",
        "            negative_prompt=neg_prompt_ft, \n",
        "            num_images_per_prompt=num_images, \n",
        "            guidance_scale=8, \n",
        "            num_inference_steps=50\n",
        "        )\n",
        "        images = result.images\n",
        "        for i, img in enumerate(images):\n",
        "            img.save(f\"data/{split}/{cls}/{cls}_{split}_{i}.png\")\n",
        "            display(img)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성 (수정 불필요)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "train_dataset = ImageFolder(\"data/train\", transform=train_transforms)\n",
        "test_dataset = ImageFolder(\"data/test\", transform=test_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "print(f\"\\n총 학습 이미지: {len(train_dataset)}, 총 테스트 이미지: {len(test_dataset)}\")\n",
        "print(f\"클래스: {train_dataset.classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ubka9fpks6-hw-v2"
      },
      "source": [
        "사전학습된 ResNet-18 모델을 불러와 출력층만 새로 정의하여 학습을 진행합니다. 기존의 특징 추출부(convolutional layers)는 가중치를 고정(freeze)하고, 분류기(fully connected layer)만 우리가 만든 '새 차 vs 폐차' 데이터셋으로 학습시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 109MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fine-tuning 시작 ---\n",
            "Epoch 1/10, Loss: 0.7768\n",
            "Epoch 2/10, Loss: 0.7793\n",
            "Epoch 3/10, Loss: 0.7030\n",
            "Epoch 4/10, Loss: 0.4569\n",
            "Epoch 5/10, Loss: 0.7558\n",
            "Epoch 6/10, Loss: 0.3228\n",
            "Epoch 7/10, Loss: 0.3301\n",
            "Epoch 8/10, Loss: 0.4057\n",
            "Epoch 9/10, Loss: 0.6480\n",
            "Epoch 10/10, Loss: 0.3418\n",
            "--- Fine-tuning 완료 ---\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "model_ft = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# 문제 11\n",
        "# [TODO] ResNet-18 모델의 특징 추출부 가중치를 고정(freeze)하는 코드를 작성하세요.\n",
        "# 힌트: for 반복문으로 model_ft.parameters()를 순회하며 param.requires_grad를 False로 설정합니다.\n",
        "for param in model_ft.parameters():\n",
        "    ... # 코드를 작성하세요\n",
        "\n",
        "# 문제 12\n",
        "# [TODO] 모델의 마지막 레이어(fc)를 우리 데이터셋의 클래스 수에 맞게 새로운 nn.Linear 레이어로 교체하는 코드를 작성하세요.\n",
        "# 힌트: model_ft.fc.in_features로 입력 뉴런 수를 가져오고, len(train_dataset.classes)로 출력 뉴런 수를 설정합니다.\n",
        "num_features = ... # 코드를 작성하세요\n",
        "model_ft.fc = ... # 코드를 작성하세요\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 문제 13 \n",
        "# [TODO] 손실 함수(CrossEntropyLoss)와 옵티마이저(Adam)를 정의하는 코드를 작성하세요.\n",
        "# 힌트: 옵티마이저는 학습할 파라미터로 model_ft.fc.parameters()만 지정해야 합니다.\n",
        "criterion = ... # 코드를 작성하세요\n",
        "optimizer = ... # 코드를 작성하세요\n",
        "\n",
        "# 문제 14\n",
        "# [TODO] 10 에포크(epoch) 동안 모델을 학습시키는 전체 학습 루프 코드를 작성하세요.\n",
        "model_ft.train()\n",
        "num_epochs = 10\n",
        "\n",
        "print(\"\\n--- Fine-tuning 시작 ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # 힌트: 아래는 일반적인 학습 5단계입니다.\n",
        "        # 1. 기울기 초기화 (optimizer.zero_grad())\n",
        "        # 2. 순전파 (outputs = model_ft(inputs))\n",
        "        # 3. 손실 계산 (loss = criterion(outputs, labels))\n",
        "        # 4. 역전파 (loss.backward())\n",
        "        # 5. 가중치 업데이트 (optimizer.step())\n",
        "        ... # 5단계에 해당하는 코드를 작성하세요\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "print(\"--- Fine-tuning 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3CkTOn0rQ3o-hw-v2"
      },
      "source": [
        "학습이 완료된 모델을 테스트 데이터셋으로 평가하여 자동차의 상태('새 차' vs '녹슨 폐차')를 얼마나 잘 분류하는지 정확도를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 테스트 시작 ---\n",
            "예측: brand_new, 정답: brand_new\n",
            "예측: brand_new, 정답: brand_new\n",
            "예측: brand_new, 정답: brand_new\n",
            "예측: rusty_abandoned, 정답: rusty_abandoned\n",
            "예측: rusty_abandoned, 정답: rusty_abandoned\n",
            "예측: rusty_abandoned, 정답: rusty_abandoned\n",
            "\n",
            "테스트 데이터셋에서 모델 정확도: 100.00 %\n"
          ]
        }
      ],
      "source": [
        "# 모델 평가 코드 (수정 불필요)\n",
        "model_ft.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "print(\"\\n--- 테스트 시작 ---\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_ft(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # 예측 결과 일부 시각화\n",
        "        for i in range(inputs.size(0)):\n",
        "            print(f\"예측: {test_dataset.classes[predicted[i]]}, 정답: {test_dataset.classes[labels[i]]}\")\n",
        "\n",
        "print(f'\\n테스트 데이터셋에서 모델 정확도: {100 * correct / total:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc9k0nR06k0P-hw-v2"
      },
      "source": [
        "# **마치며**\n",
        "\n",
        "이번 과제를 통해 텍스트-투-이미지 생성, CLIP을 이용한 멀티모달 평가, CNN 모델과의 비교, 그리고 생성 데이터를 활용한 Fine-tuning까지 파운데이션 모델 활용의 전 과정을 직접 구현해보았습니다. 특히 단순 객체 분류를 넘어, 객체의 '상태'를 분류하는 모델을 만들며 프롬프트 엔지니어링의 중요성과 생성 데이터의 활용 가능성을 깊이 있게 탐색하는 계기가 되었기를 바랍니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
